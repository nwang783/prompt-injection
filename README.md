# Prompt Injection Tests

This repository explores and tests common techniques for prompt injection attacks in AI systems, focusing on vulnerabilities and mitigation strategies.

## Overview
Prompt injection is a method used to manipulate the output of AI models by crafting inputs that override their intended behavior. This repository contains scripts designed to:

- Simulate basic and advanced prompt injection techniques.
- Test AI models' resilience against such attacks.
- Provide insights into improving prompt safety.

## Contents
- `test_basic_prompt_injection.py`: Demonstrates a straightforward prompt injection attack.
- `test_advanced_prompt_injection.py`: Explores a more complex prompt injection scenario.

## Usage
1. Clone this repository:
   ```bash
   git clone https://github.com/nwang783/prompt-injection
   ```
2. Navigate to the repository directory:
   ```bash
   cd prompt-injection
   ```
3. Run the scripts:
   ```bash
   python test_basic_prompt_injection.py
   python test_advanced_prompt_injection.py
   ```

## Contribution
Got a new prompt injection scenario or mitigation strategy? Feel free to fork this repo and submit a pull request!

## License
This project is open-source and available under the MIT License.

