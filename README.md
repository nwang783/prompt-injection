# Prompt Injection Tests

This repository explores and tests common techniques for prompt injection attacks in AI systems, focusing on vulnerabilities and mitigation strategies.

# Prompt Injection Testing Repository

## Overview
This repository is dedicated to testing and understanding prompt injection techniques. Prompt injection is a critical challenge in AI safety and security, where malicious input can hijack an AI's behavior. These scripts aim to simulate and explore vulnerabilities to better understand and mitigate such risks.

## Included Scripts
1. **`test_basic_prompt_injection.py`**
   - Demonstrates a simple prompt injection example to illustrate how malicious inputs can override initial prompts.

2. **`test_advanced_prompt_injection.py`**
   - Explores a more advanced injection scenario where the AI might inadvertently reveal sensitive data.

3. **`complex_prompt_injection_simulation.py`**
   - Simulates sophisticated prompt injection attacks involving dynamic payloads to demonstrate potential vulnerabilities in more complex AI systems.

## Usage
1. Clone this repository:
   ```bash
   git clone https://github.com/nwang783/prompt-injection.git
   ```
2. Navigate to the repository directory:
   ```bash
   cd prompt-injection
   ```
3. Run the scripts:
   ```bash
   python test_basic_prompt_injection.py
   python test_advanced_prompt_injection.py
   ```

## Contribution
Got a new prompt injection scenario or mitigation strategy? Feel free to fork this repo and submit a pull request!

## License
This project is open-source and available under the MIT License.

